{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q1.*** What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used in machine learning to transform features to a specific range, typically between 0 and 1. It is also known as Min-Max normalization. The formula to perform Min-Max scaling on a feature x is:\n",
    "\n",
    "X(normalised) = (x - min(x))/(max(x)-min(x))\n",
    "\n",
    "Min-Max scaling is beneficial when the features in the dataset have different scales, and some machine learning algorithms, like neural networks, perform better when input features are within a similar range. By scaling the features to a specific range, you can ensure that no particular feature dominates the others during the training of the model.\n",
    "\n",
    "\n",
    "***Example*** \n",
    "\n",
    "Let's say you have a dataset containing the following ages: 20, 25, 30, and 35 years. To perform Min-Max scaling on these ages to a range between 0 and 1:\n",
    "\n",
    "So, after Min-Max scaling, the ages 20, 25, 30, and 35 are transformed to 0, 0.25, 0.5, and 1 respectively, and they now fall within the range of 0 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q2.*** What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit vector technique, also known as Unit Vector Scaling or Normalization, is a feature scaling method where each feature is scaled to have a unit norm (length 1). This technique is often used in machine learning when the direction of the data is more important than its magnitude. It is particularly useful in algorithms that involve measuring distances, like clustering algorithms or when using techniques such as cosine similarity.\n",
    "\n",
    "The formula to calculate unit vector for a feature vector is :- \n",
    "\n",
    "X(normalized) = X / (||X||)\n",
    "\n",
    "\n",
    "Let's consider a dataset with two features represented by vectors:X1=[3,4] and X2=[1,2].\n",
    "After unit vector scaling, the vectors X1 and X2 are transformed into unit vectors [0.6,0.8] ans [0.45,0.89] respectively,meaning they both have a length of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q3.*** What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Principal Component Analysis (PCA)*** is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional form. It does so by identifying the principal components in the data, which are the directions in which the data varies the most.\n",
    "\n",
    "Here's how PCA works step by step:\n",
    "\n",
    "Standardize the data: If the features in the dataset are measured in different units or have different scales, it's important to standardize the data (subtract the mean and divide by the standard deviation for each feature) so that they all have a similar scale.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data. The covariance matrix shows how different features vary with respect to each other.\n",
    "\n",
    "Compute eigenvalues and eigenvectors: Calculate the eigenvalues and eigenvectors of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues represent the magnitude of the variance in each of those components.\n",
    "\n",
    "Sort eigenvalues and select principal components: Sort the eigenvalues in descending order. The eigenvectors corresponding to the largest eigenvalues are the principal components. You can choose the top k eigenvectors to form a k-dimensional subspace \n",
    "(where k is the number of dimensions you want to reduce the data to).\n",
    "\n",
    "Project the data onto the new subspace: Multiply the original data by the selected eigenvectors to obtain the new lower-dimensional representation of the data.\n",
    "\n",
    "\n",
    "## Here's an example to illustrate PCA:\n",
    "\n",
    "Let's say you have a dataset with two features: height in inches and weight in pounds for a group of individuals. You want to reduce this data to one dimension using PCA.\n",
    "\n",
    "Standardize the data: Assume the mean height is 68 inches, and the mean weight is 150 pounds. Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "Compute eigenvalues and eigenvectors: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Sort eigenvalues and select principal components: Suppose the eigenvalue corresponding to the first principal component is 1.5 and the eigenvalue corresponding to the second principal component is 0.8. Since the first eigenvalue is larger, you select the corresponding eigenvector as the principal component.\n",
    "\n",
    "Project the data onto the new subspace: Multiply the original data by the selected eigenvector to obtain the new one-dimensional representation of the data.\n",
    "\n",
    "In this simplified example, PCA has reduced the two-dimensional data (height and weight) to one dimension, capturing the most significant variation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q4.*** What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique used for both dimensionality reduction and feature extraction. The relationship between PCA and feature extraction lies in the fact that PCA identifies new features (principal components) that are linear combinations of the original features. These new features are orthogonal and capture the most significant patterns in the data. By selecting a subset of these principal components, you can perform feature extraction, transforming the original features into a smaller set of uncorrelated features that retain most of the essential information of the dataset.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "### Steps for PCA-based Feature Extraction:\n",
    "\n",
    "1. **Standardize the Data**: Standardize the original features to have zero mean and unit variance.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. **Sort Eigenvalues and Select Principal Components**: Sort the eigenvalues in descending order. Select the top \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues to form the transformation matrix.\n",
    "\n",
    "5. **Transform the Original Features**: Multiply the original feature matrix by the selected \\(k\\) eigenvectors to obtain the new feature matrix with reduced dimensions.\n",
    "\n",
    "Here's an example to illustrate PCA-based feature extraction:\n",
    "\n",
    "Let's consider a dataset with three features: age, income, and education level. We want to perform feature extraction using PCA to reduce the dimensionality to two features.\n",
    "\n",
    "1. **Standardize the Data**: Assume the features are standardized (mean=0, variance=1) for simplicity.\n",
    "\n",
    "2. **Compute Covariance Matrix**: Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**: Compute the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. **Sort Eigenvalues and Select Principal Components**: Suppose the eigenvalues are [2.5, 0.8, 0.3] in descending order. We want to reduce the dimensionality to two features, so we select the top two eigenvectors corresponding to the largest eigenvalues.\n",
    "\n",
    "   Transformation Matrix:\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   0.6 & 0.7 \\\\\n",
    "   0.4 & -0.5 \\\\\n",
    "   0.7 & 0.4 \\\\\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "5. **Transform the Original Features**: Multiply the original feature matrix by the selected eigenvectors:\n",
    "\n",
    "   Original Feature Matrix:\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   25 & 50000 & 16 \\\\\n",
    "   30 & 60000 & 18 \\\\\n",
    "   35 & 75000 & 20 \\\\\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "   New Feature Matrix (after transformation):\n",
    "   \\[\n",
    "   \\begin{bmatrix}\n",
    "   46000 & 2.5 \\\\\n",
    "   56000 & 3.0 \\\\\n",
    "   71000 & 2.8 \\\\\n",
    "   \\end{bmatrix}\n",
    "   \\]\n",
    "\n",
    "In this example, PCA has extracted two features that capture the most significant patterns in the original data. The first new feature combines age, income, and education level in a specific way, and the second feature captures additional variation. These two features can now be used for further analysis or modeling, reducing the dimensionality of the data while preserving essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q5.*** You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be a valuable preprocessing technique to standardize the features like price, rating, and delivery time. Here's how you can use Min-Max scaling for this purpose:\n",
    "\n",
    "1. **Understanding the Features**:\n",
    "   - **Price**: It might be represented in a numerical format, such as dollars. Prices can vary significantly, and Min-Max scaling can help in bringing them to a similar scale.\n",
    "   - **Rating**: Ratings are typically on a scale, say from 1 to 5. Even though they are on the same scale, scaling can still be beneficial for consistency.\n",
    "   - **Delivery Time**: Delivery time might be represented in minutes. Like price, delivery times can vary widely and need to be scaled for uniformity.\n",
    "\n",
    "2. **Min-Max Scaling**:\n",
    "   - **Identify the Range for Each Feature**:\n",
    "     - For **price**, suppose the prices range from $5 to $50.\n",
    "     - For **rating**, the scale is 1 to 5.\n",
    "     - For **delivery time**, it might range from 15 minutes to 90 minutes.\n",
    "   - **Apply Min-Max Scaling**:\n",
    "     - For each feature, apply the Min-Max scaling formula to transform the values into a range between 0 and 1. The formula for Min-Max scaling was mentioned earlier:\n",
    "\n",
    "      X(normalised) = (x-x(min))/(x(max)-x(min))\n",
    "\n",
    "     - For instance, if you want to scale the price of a food item priced at $20, the scaled value would be:\n",
    "\n",
    "     x{normalized} = {20 - 5}/{50 - 5} = {15}/{45} = 0.3333 \n",
    "\n",
    "   - **Repeat the Process for Other Features**: Apply the same scaling procedure for the rating and delivery time.\n",
    "\n",
    "3. **Result**:\n",
    "   - After Min-Max scaling, all the features (price, rating, and delivery time) are transformed into a common scale between 0 and 1. Now, they are ready to be used as input for your recommendation system algorithm.\n",
    "\n",
    "By employing Min-Max scaling, you ensure that all the features have equal weight in your recommendation system, regardless of their original scales. This can be crucial, especially if you're using algorithms that rely on distance calculations, such as collaborative filtering, where the scale of features can significantly impact the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q6.*** You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset for predicting stock prices can be beneficial, especially when dealing with a large number of features. Here's how you can apply PCA in the context of your stock price prediction project:\n",
    "\n",
    "### Step 1: Data Preprocessing\n",
    "\n",
    "Before applying PCA, it's crucial to preprocess the data:\n",
    "\n",
    "1. **Handling Missing Values**: Address any missing or null values in the dataset using techniques like imputation or removal of incomplete data points.\n",
    "\n",
    "2. **Standardization**: Standardize the features to give them a mean of 0 and a standard deviation of 1. Standardization ensures that all features are on a similar scale, which is a prerequisite for PCA.\n",
    "\n",
    "### Step 2: Applying PCA\n",
    "\n",
    "1. **Calculate Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix. The covariance matrix represents the relationships between different features.\n",
    "\n",
    "2. **Compute Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix. These eigenvectors represent the principal components, and eigenvalues indicate their magnitude.\n",
    "\n",
    "3. **Sort Eigenvalues**: Sort the eigenvalues in descending order. The higher the eigenvalue, the more variance the corresponding principal component explains.\n",
    "\n",
    "4. **Select Principal Components**: Choose the top \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues to form the transformation matrix. The value of \\(k\\) can be determined based on the cumulative explained variance ratio. For instance, you might select \\(k\\) such that 95% of the variance is retained.\n",
    "\n",
    "5. **Transform the Data**: Multiply the original standardized feature matrix by the selected \\(k\\) eigenvectors to obtain the reduced-dimensional feature matrix.\n",
    "\n",
    "### Step 3: Model Training and Prediction\n",
    "\n",
    "1. **Split Data**: Split the reduced-dimensional feature matrix and corresponding target values into training and testing sets.\n",
    "\n",
    "2. **Train Model**: Train your stock price prediction model (such as regression or a time-series model) using the reduced-dimensional feature matrix in the training set.\n",
    "\n",
    "3. **Evaluate Model**: Evaluate the model's performance using the testing set. Metrics like mean squared error (MSE) or root mean squared error (RMSE) can be used to assess the prediction accuracy.\n",
    "\n",
    "### Benefits of PCA in Stock Price Prediction:\n",
    "\n",
    "- **Dimensionality Reduction**: PCA reduces the number of features, which can help mitigate the curse of dimensionality and improve the model's generalization.\n",
    "  \n",
    "- **Noise Reduction**: By focusing on the principal components with the highest variance, PCA helps in filtering out noise and retaining the essential patterns in the data.\n",
    "\n",
    "- **Visualization**: Reduced dimensionality allows for easier visualization of the data, which can aid in understanding the underlying relationships.\n",
    "\n",
    "- **Speed Up Training**: With fewer dimensions, the model training process is faster, enabling quicker experimentation with different algorithms and hyperparameters.\n",
    "\n",
    "Remember that while PCA can be a powerful tool, it's essential to strike a balance. Reducing dimensionality too aggressively might lead to loss of important information, impacting the predictive power of your model. Experimentation and careful evaluation of the model's performance at different dimensions are crucial in determining the optimal number of principal components to retain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q7.*** For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1,5,10,15,20])\n",
    "mini = np.min(data)\n",
    "maxi = np.max(data)\n",
    "# mini,maxi \n",
    "min_max_data = []\n",
    "for i in data:\n",
    "    scaled_data = ((i-mini)/(maxi-mini))*2-1\n",
    "    min_max_data.append(scaled_data)\n",
    "min_max_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q8*** For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision of how many principal components to retain in PCA involves balancing the need for dimensionality reduction with the preservation of as much variance as possible. A common approach to decide the number of principal components to keep is by examining the explained variance ratio.\n",
    "\n",
    "The explained variance ratio tells us the proportion of the dataset's variance that lies along the axes of each principal component. Retaining a higher percentage of explained variance ensures that the retained components capture most of the dataset's variability. A common threshold is to retain principal components that collectively explain, for example, 95% or 99% of the total variance.\n",
    "\n",
    "Let's consider the steps for PCA and then discuss how to decide the number of principal components to retain for your dataset:\n",
    "\n",
    "### Steps for PCA:\n",
    "\n",
    "1. **Standardize the Data**: Standardize the features (height, weight, age, blood pressure) to have zero mean and unit variance.\n",
    "\n",
    "2. **Calculate Covariance Matrix**: Compute the covariance matrix of the standardized feature matrix.\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "4. **Sort Eigenvalues**: Sort the eigenvalues in descending order.\n",
    "\n",
    "5. **Decide Number of Principal Components to Retain**:\n",
    "   - Calculate the explained variance ratio for each principal component: \\( \\text{explained variance ratio} = \\frac{\\text{eigenvalue}}{\\sum \\text{all eigenvalues}} \\).\n",
    "   - Accumulate the explained variance ratios.\n",
    "   - Decide the number of principal components (\\(k\\)) to retain based on a threshold (e.g., 95% or 99% total variance explained).\n",
    "\n",
    "6. **Select Principal Components and Transform Data**: Select the top \\(k\\) eigenvectors corresponding to the \\(k\\) largest eigenvalues. Multiply the original standardized feature matrix by these \\(k\\) eigenvectors to obtain the reduced-dimensional feature matrix.\n",
    "\n",
    "### Decision on Number of Principal Components:\n",
    "\n",
    "For example, if you find that the first three principal components explain 98% of the total variance, you might decide to retain these three components. This means you are reducing the dimensionality from 5 features to 3, capturing 98% of the dataset's variability.\n",
    "\n",
    "The choice of the threshold (95%, 99%, etc.) depends on your specific use case and the trade-off between dimensionality reduction and information preservation. Retaining more principal components preserves more information but might lead to higher computational complexity. Conversely, retaining fewer components might result in information loss.\n",
    "\n",
    "In practice, it's common to start with a threshold like 95% and analyze the results. If the model's performance is not satisfactory, you can experiment with retaining more components to capture additional variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
